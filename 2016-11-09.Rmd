---
title: "STA257"
author: "Neil Montgomery | HTML is official | PDF versions good for in-class use only"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  ioslides_presentation: 
    css: '../styles.css' 
    widescreen: true 
    transition: 0.001
header-includes:
- \usepackage{cancel}
---
\newcommand\given[1][]{\:#1\vert\:}
\newcommand\P[1]{P{\left(#1\right)}}
\newcommand\F[1]{F_{\tiny{#1}}}
\newcommand\f[1]{f_{\tiny{#1}}}
\newcommand\p[1]{p_{\tiny{#1}}}



## from last time 

Theorem: $X \perp Y$ if and only if the joint cdf $F(x,y) = \F{X}(x)\F{Y}(y)$ is the product of the marginal cdfs.

Proof: $\Longleftarrow$ ("only if") too hard; $\Longrightarrow$ left as exercise.

Corollary: $X \perp Y$ if and only if the joint $f(x,y) = \f{X}(x)\f{Y}(y)$

To verify, in practice check two things:

1. The density factors. **Note: enough to factor into a function of $x$ and a function of $y.$**
2. The non-zero region is a rectangle (possibly infinite in either direction.) **Note: technically a "cross product" is all that is needed, but in almost all practical cases it will be a rectangle.**

## other important independence results (advanced) { .build }

Theorem: If $X$ and $Y$ are independent, so are $g(X)$ and $h(Y)$ for any* functions $g$ and $h$.

Sketch of proof: ...

Definition of independence extends to any number of random variables. We say $X_1, X_2,\ldots,X_n$ are independent if:
$$P(X_1 \in A_1, X_2 \in A_2, \ldots, X_n \in A_n) = P(X_1 \in A_1)\cdots P(X_n \in A_n)$$

for any* subsets $A_i \in \mathbb{R}$.

## conditional distributions

Recall the sum $X$ and the absolute difference $X$ of two dice:

![](xy.PNG)

## discrete case { .build }

Given a joint pmf for $X$ and $Y$ denoted by $p(x, y)$, define:
$$\p{X|Y}(x|y) = \begin{cases}
\frac{p(x,y)}{\p{Y}(y)} &: \text{ where } \p{Y}(y) > 0\\
0 &: \text{ otherwise }\end{cases}$$
For any fixed $Y$ with $\p{Y}(y) > 0$, this is a valid pmf.

This pmf describes what is called "the conditional distribution of $X$ given $Y = y.$"

Useful result:
$$\begin{align*}
p(x,y) &= \p{X|Y}(x|y)\p{Y}(x)\\
\p{X}(x) &= \sum\limits_{y} \p{X|Y}(x|y)\p{Y}(y)\end{align*}$$

## classic example { .build }

At home my phone rings $Y$ times with $Y\sim\text{Poisson}(\lambda)$ in one hour.I answer the phone with probability $p$ when it rings. What is the distribution of the $X$, the number of times I answer the phone in an hour?

(In fact $p$ so $X=0$ always. Note for people not in attendance...this is a joke about me not answering the phone.)

## continuous case { .build }

The concept is similar. We examine a "slice" of the joint density at, say $X=x$ and consider the distribution of $Y$ at that fixed value of $x$.

The *conditional density of $Y$ given $X=x$* is defined as:
$$\f{Y|X}(y|x) = \frac{f(x,y)}{\f{X}(x)}$$
wherever $\f{X}(x) > 0$. *Note: what is meant by this is $\f{X|Y}$ is only even defined whenever $\f{X}(x)>0.$ One still needs to carefully consider the support of $\f{Y|X}.$*

Examples:

1. $f(x, y) = \frac{1}{\pi}$ on $x^2 + y^2 \le 1$.
2. $f(x,y) = \lambda^2e^{-\lambda y}$ on $0 < x < y$.

